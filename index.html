<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enhancing Chest X-ray Zero-shot Classification via Extended Vision-Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <meta name="description" content="Enhancing Chest X-ray Zero-shot Classification via Extended Vision-Language Models Thesis">
  <meta property="og:title" content="Enhancing Chest X-ray Zero-shot Classification via Extended Vision-Language Models"/>
  <meta property="og:description" content="Enhancing Chest X-ray Zero-shot Classification via Extended Vision-Language Models Thesis"/>
  <meta property="og:url" content="https://mimicpy.github.io/"/>

  <meta property="og:image" content="./static/images/contrastive.png" />
  <meta property="og:image:width" content="2446"/>
  <meta property="og:image:height" content="1438"/>

  <meta name="twitter:title" content="Enhancing Chest X-ray Zero-shot Classification via Extended Vision-Language Models">
  <meta name="twitter:description" content="Thesis Website">

  <meta name="twitter:image" content="./static/images/contrastive.png">
  <meta name="twitter:card" content="Thesis Website">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/fav.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enhancing Chest X-ray Zero-shot Classification via Extended Vision-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span>Gabriel M. de Oliveira</span><sup>1, 2</sup>,</span>
            <span class="author-block">
              <span>Er Jin</span><sup>1</sup>,</span>
            <span class="author-block">
              <span>Johannes Stegmaier</span><sup>1</sup>,
            </span>
            <span class="author-block">
              <span>Edson S. Gomi</span><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>RWTH Aachen</span>
            <span class="author-block"><sup>2</sup>Escola Politécnica da Universidade de São Paulo</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">  
        <figure style="text-align: center;">
          <img id="teaser" width="100%" src="static/images/cxrviews.png">  
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a framework for finetuning CLIP for the medical concept domain, 
            extending CXR-CLIP's previous work and showing optimizations that yield better
            retrieval and classification results.
          </p>
          <p>
            Although multimodal models like Contrastive Language-Image Pretraining
            excel in general tasks, previous work has shown that two major issues: a significant
            modality gap and persistent dataset bias, still limit their effectiveness in specialized
            domains like medical imaging. This thesis aims to enhance the performance of the
            SOTA method, CXR-CLIP, in Chest X-Ray analysis by addressing data
            inconsistencies and biases and closing the modality gap, which are also affected by
            structural inconsistencies in their treatment of the MIMIC-CXR dataset, which hinder
            the alignment of vision-language embeddings.
          </p>
          <p>
            To overcome these challenges, this
            work investigates the gap in CXR-CLIP’s encoders, propose an alternative to its
            backtranslation method to reduce hallucination and skewed bias, and explore well-
            documented fine-tuning techniques, namely Low-Rank Adaptation and Visual
            Prompt Tuning, to mitigate the modality gap under resource constraints. This
            study’s findings reveal the proposed framework significantly enhances the model’s
            performance, increasing top-1 retrieval recall by over 10%, and average classification score by 3%, compared to the SOTA.
            This work provides valuable insights into medical imaging and explainable Artificial
            Intelligence, contributing to future development paths in the area.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Background</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified"> 
          <p>
            Vision-language models for CXR are a current focal point of research in medical imaging.
            By combining visual data from medical scans with textual data from radiology
            reports, these models offer the potential to improve diagnostic accuracy and enable
            more effective data retrieval.

            CXR-CLIP is a pretraining framework built upon the original
            CLIP implementation, designed to enhance its zero-shot capabilities on infrequent concept
            domains, such as Chest X-Ray classification.

          </p>
          <p>
            Instead of the traditional one image - one text pair method from CLIP, CXR-CLIP proses using
            two images and two texts per study, which are obtained by applying multiple views from a CXR
            study and by decomposing its reports in subsections.

            <figure style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/cxrclipbatch.png">  
            </figure>

            When a multiview study is not possible, CXR-CLIP applies default image transformations to augment
            a single image into a corresponding pair. It also tries to follow the same logic by augmenting
            its report when multiple sections are not available, but by using backtranslation via 
            <a href="https://huggingface.co/Helsinki-NLP/opus-mt-en-it">Opus-MT</a>.

            <figure style="text-align: center;">
              <img id="teaser" width="75%" src="static/images/backtranslation.png">  
            </figure>

            Finally, it expands on the usual CLIP loss (computed for each pair of image and text) by adding two terms: 
            an inter-text (between the two texts) loss term and an inter-image (between the two images) loss term. 

            <figure style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/cxrcliplosses.png">  
            </figure>
          </p>
        </div>  
        <h2 class="title is-4">Limitations</h2>   
        <div class="content has-text-justified">
          <p>
            Extensive exploration of the CXR-CLIP model led to the identification of two major problems.
            <ul type="1">
              <li>
                <b>Modality Gap</b>: 
                <span style="font-size: 95%;">
                  Semantic and structural differences in the <i>CLIP encoders</i> create a geometric misalignment between
                  the embedding spaces of each modal. This separation weakens the performance of cross-modal tasks such as zero-shot
                  retrieval.
                </span>

                <figure style="text-align: center;">
                  <img id="teaser" width="75%" src="static/images/gap.png">  
                </figure>
              </li>

              <li>
                <b>Backtranslation Hallucinates</b>: 
                <span style="font-size: 95%;">
                  In the case of radiology reports, backtranslation often leads to the
                  distortion or even corruption of critical clinical information, which can severely affect
                  the integrity of the dataset.
                </span>

                <figure style="text-align: center;">
                  <img id="teaser" width="75%" src="static/images/backtranslation-probs.png">  
                </figure>
              </li>
            </ul>  
          </p>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Methods</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Datasets</h2>
        <div class="content has-text-justified"> 
          <p>
            Chest Radiography is one of the most widely conducted radiological
            procedures globally, playing a critical role in both routine medical evaluations and
            diagnostics. It involves the creation of static images by passing X-rays
            through a patient’s thoracic region, enabling the visualization of internal structures,
            such as the lungs, heart, and bones. An important feature of CXR is the possibility of capturing images from various
            angles or positions, referred to as projections

            <figure style="text-align: center;">
              <img id="teaser" width="75%" src="static/images/rad_positions.png">  
              <figcaption>
                The varied positions for CXR: anteroposterior (AP), posteroanterior (PA) and lateral. 
                Notice that the positions are named after the relative angle from the body to the receiving apparatus.
              </figcaption>
            </figure>

            Based on CXR-CLIP's methods, the following datasets were used:
            <ul type="1">
              <li><b>MIMIC-CXR</b>: <span style="font-size: 95%;"> a large, publicly available collection of de-identified
                chest radiographs in DICOM format, accompanied by free-text radiology reports. This dataset consists of 377.110 images, 
                corresponding to 227.835 unique radiographic studies performed at the Beth Israel Deaconess Medical Center in Boston,
                Massachusett. Used in both <i>finetuning</i> (official training and validation splits) and retrieval <i>evaluation</i> (official test split).</span></li>
              
              <li><b>Open-I</b>: <span style="font-size: 95%;"> consists of 3.996 radiology reports from
                the Indiana Network for Patient Care and 8.121 associated images. The images
                and reports were de-identified automatically. From each study, one of the report
                sections and one frontal-view image were sampled and used for image-to-text retrieval
                evaluation. Used exclusively in retrieval <i>evaluation</i> (official test split).</span></li>

              <li><b>VinDr-CXR</b>: <span style="font-size: 95%;"> consists of 18.000 PA view CXR scans that come with both the localization of critical 
                findings and the classification of common thoracic diseases. Samples were retrospectively collected from the Hospital 108 and the Hanoi Medical 
                University Hospital, two of the largest hospitals in Vietnam. Used exclusively in classification <i>evaluation</i> (official test split).</span></li>
              
              <li><b>RSNA Pneumonia</b>: <span style="font-size: 95%;"> originally from a <a href="https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/overview">Kaggle competition</a> hosted by
                the Radiological Society of North America (RSNA), contains 26.684 deidentified CXR views, annotated by either a pneumonia or no pneumonia label. Used exclusively in classification <i>evaluation</i> (official test split).</span></li>
              
              <li><b>SIIM Pneumothorax</b>: <span style="font-size: 95%;"> originally from a <a href="https://www.kaggle.com/competitions/siim-acr-pneumothorax-segmentation/overview">Kaggle competition</a> hosted by
                the Society for Imaging Informatics in Medicine (SIIM), contains 12.047 deidentified CXR views, annotated with either no mask or a binary encoded
                indicating pneumothorax (collapsed lung). Used exclusively in classification <i>evaluation</i> (official test split).</span></li>
            </ul>  
          </p>
        </div>         
      </div>
    </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Framework</h2>
        <div class="content has-text-justified"> 
          <p>
            The core principle of the CXR-CLIP framework revolves around leveraging multiple views of chest X-rays and 
            corresponding sections of radiology reports during training. Given a pretrained <a href="https://huggingface.co/microsoft/swin-tiny-patch4-window7-224">Swin-Tiny</a> 
            image encoder and a domain-specialized pretrained <a href="https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT">BioClinicalBERT</a>, the following
            methods are considered: 
            <ul type="1">
              <li><b>Summarization instead of Backtranslation</b>. <span style="font-size: 95%;">
                Unlike backtranslation, which can introduce errors and hallucinate content, 
                summarization condenses the original text while preserving its essential meaning 
                and clinical details. We use a summarization-tuned <a href="https://huggingface.co/Falconsai/text_summarization">T5-Small</a> 
                variant to summarize training dataset entries.</span>
                <figure style="text-align: center;">
                  <img id="teaser" width="50%" src="static/images/summ.png">  
                </figure></li>
              <li><b>Parameter-Efficient Finetuning</b>. <span style="font-size: 95%;">Instead of pretraining or full-finetuning we employ parameter-efficient
                finetuning methods, namely:
                <ul type="1">
                  <li> <b>Low-Rank Adaptation (LoRA)</b>: the update to each weight matrix in attention layers is proxied into a product of 
                    low rank matrices, which receive gradient steps. 
                  
                    <figure style="text-align: center;">
                      <img id="teaser" width="50%" src="static/images/lora.png">  
                    </figure></li>
                  <li> <b>Visual Prompt Tuning (VPT)</b>: a set of learnable prompt tokens is appended to each encoder layer. The layers are kept frozen
                  and only prompt tokens receive gradient updates.
                
                  <figure style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/vptsd.png">  
                  </figure></li>
                </span>
                </ul>  
            </ul>  
          </p>
        </div>         
      </div>
    </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Results</h2>
    </div>
  </div>


  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Model Zoo</h2>
        <div class="content has-text-justified">
          <p>
            Various augmentation and finetuning techniques were introduced. In combination, these techniques yield 
            several model variants based on the different possible configurations of finetuning applied to the CXR-CLIP 
            baseline:
          </p>
          <table class="table is-fullwidth is-hoverable" style="text-align: center">
            <thead>
              <tr>
                <th colspan="2"></th>
                <th colspan="2">LoRA</th>
                <th colspan="2">Prompt Tuning</th>
              </tr>
              <tr>
                <th style="text-align: left">Model Variant</th>
                <th>Summarization</th>
                <th>Vision</th>
                <th>Language</th>
                <th>Vision</th>
                <th>Language</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <th style="text-align: left">Baseline</th>
                <td>✘</td>
                <td>✘</td>
                <td>✘</td>
                <td>✘</td>
                <td>✘</td>
              </tr>
              <tr>
                <th style="text-align: left">LoRA-V</th>
                <td>✓</td>
                <td>✓</td>
                <td>✘</td>
                <td>✘</td>
                <td>✘</td>
              </tr>
              <tr>
                <th style="text-align: left">LoRA-T</th>
                <td>✓</td>
                <td>✘</td>
                <td>✓</td>
                <td>✘</td>
                <td>✘</td>
              </tr>
              <tr>
                <th style="text-align: left">LoRA-TV</th>
                <td>✓</td>
                <td>✓</td>
                <td>✓</td>
                <td>✘</td>
                <td>✘</td>
              </tr>
              <tr>
                <th style="text-align: left">Prompt-T</th>
                <td>✓</td>
                <td>✘</td>
                <td>✘</td>
                <td>✓</td>
                <td>✘</td>
              </tr>
              <tr>
                <th style="text-align: left">Prompt-V</th>
                <td>✓</td>
                <td>✘</td>
                <td>✘</td>
                <td>✘</td>
                <td>✓</td>
              </tr>
              <tr>
                <th style="text-align: left">Prompt-TV</th>
                <td>✓</td>
                <td>✘</td>
                <td>✘</td>
                <td>✓</td>
                <td>✓</td>
              </tr>
              <tr>
                <th style="text-align: left">Hybrid</th>
                <td>✓</td>
                <td>✘</td>
                <td>✓</td>
                <td>✓</td>
                <td>✘</td>
              </tr>
            </tbody>
          </table>
          <p>
            For all models, we set LoRA rank to 256 for language encoder and 32 for vision encoder.
            Similarly, the number of prompts is set to 8 for language encoder and 2 for vision encoder. 
          </p>
        </div>
        <h2 class="title is-4">Image-Text Retrieval</h2>
        <div class="content has-text-justified"> 
          <p>
            For each image in a batch, we query the cosine similarity between its embedding and
            all text embeddings in the batch. These similarities are then ranked. For each image,
            if the embedding of its perfect pair lies on the top K entries in the ranking, we score
            one point to its top-K retrieval recall (<b>R@K</b>), which, in the end, is normalized by the batch size.
          </p>
          <table class="table is-fullwidth is-hoverable" style="text-align: center">
            <thead style="border-collapse: collapse;">
              <tr>
                <th></th>
                <th colspan="3">MIMIC-CXR</th>
                <th colspan="3">Open-I</th>
              </tr>
              <tr>
                <th style="text-align: left">Model Variant</th>
                <th>R@1</th>
                <th>R@5</th>
                <th>R@10</th>
                <th>R@1</th>
                <th>R@5</th>
                <th>R@10</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <th style="text-align: left">Baseline</th>
                <td>21.6</td>
                <td>48.9</td>
                <td>60.2</td>
                <td>3.8</td>
                <td>8.3</td>
                <td>12.3</td>      
              </tr>
              <tr class="is-selected">
                <th style="text-align: left">LoRA-V</th>
                <td><b>32.0</b></td> 
                <td>50.8</td> 
                <td><b>61.2</b></td> 
                <td>5.1</td> 
                <td>9.1</td> 
                <td>12.7</td>
              </tr>
              <tr>
                <th style="text-align: left">LoRA-T</th> 
                <td>30.8</td> 
                <td>49.2</td> 
                <td>60.4</td> 
                <td>4.5 </td> 
                <td>8.6 </td> 
                <td>12.1</td>
              </tr>
              <tr>
                <th style="text-align: left">LoRA-TV</th>
                <td>31.8</td> 
                <td><b>50.9</b></td> 
                <td>61.0</td> 
                <td>5.2 </td> 
                <td>9.3 </td> 
                <td>13.0</td>
              </tr>
              <tr>
                <th style="text-align: left">Prompt-T</th> 
                <td>29.9 </td> 
                <td>48.8 </td> 
                <td>59.4 </td> 
                <td>4.5 </td> 
                <td>8.4 </td> 
                <td>11.3</td>
              </tr>
              <tr>
                <th style="text-align: left">Prompt-V</th>    
                <td>26.3</td> 
                <td>43.8</td> 
                <td>54.0</td> 
                <td>3.7 </td> 
                <td>7.5 </td> 
                <td>10.6</td>
              </tr>
              <tr>
                <th style="text-align: left">Prompt-TV</th>    
                <td>22.7</td> 
                <td>39.7</td> 
                <td>49.9</td> 
                <td>3.3 </td> 
                <td>6.3 </td> 
                <td>8.5</td>
              </tr>
              <tr>
                <th style="text-align: left">Hybrid</th>    
                <td>29.3</td> 
                <td>47.3</td> 
                <td>58.4</td> 
                <td>4.1 </td> 
                <td>8.2 </td> 
                <td>11.6</td>
              </tr>
            </tbody>
          </table>
          <p>
            Given these observations, only LoRA models will be used for next experiments.
          </p>
        </div>
        <h2 class="title is-4">Zero-Shot Binary Classification</h2>
        <div class="content has-text-justified">
          <p>
            It is possible to use the embeddings directly to craft a binary classifier (is a class present or not). To do so,
            the embedding of an image is compared to class-specific positive and negative prompts, e.g. for pneumonia 
            <span class="is-family-monospace">[No evidence of pneumonia, Findings suggesting pneumonia]</span>. The prompt
            with most similarity will then define the result of classification. Therefore, classification metrics might be used in
            particular, AUROC is presented.          
          </p>
          <table class="table is-fullwidth is-hoverable" style="text-align: center">
            <thead style="border-collapse: collapse;">
              <tr>
                <th style="text-align: left">Model Variant</th>
                <th>VinDr-CXR</th>
                <th>RSNA Pneumonia</th>
                <th>SIIM Pneumothorax</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <th style="text-align: left">Baseline</th>
                <td></td>
                <td></td>
                <td></td>   
              </tr>
              <tr>
                <th style="text-align: left">LoRA-V</th>
                <td></td> 
                <td></td> 
                <td></td> 
              </tr>
              <tr>
                <th style="text-align: left">LoRA-T</th> 
                <td></td> 
                <td></td> 
                <td></td> 
              </tr>
              <tr>
                <th style="text-align: left">LoRA-TV</th>
                <td></td> 
                <td></td> 
                <td></td> 
              </tr>
            </tbody>
          </table>
          <p>
            Particularly, no sample from the evaluation dataset is used, since the performance of zero-shot classification is desired.       
          </p>
          </div>
        </div> 
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
  <!-- Concurrent Work. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Related Links</h2>

      <div class="content has-text-justified">
        <p>
          Our work was developed after the methods from <a href="https://arxiv.org/abs/2310.13292">CXR-CLIP</a>. 
          We acknowledge their contribution in keeping their project open source.
        </p>
      </div>
    </div>
  </div>
  <!--/ Concurrent Work. -->
</div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
    
          <p>
    <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only.
    </p>
    
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
